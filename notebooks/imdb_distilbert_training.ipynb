{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT IMDb Sentiment Fine-Tuning Recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT compresses the knowledge of BERT into a lighter, faster model—perfect for production inference.  \n",
    "This notebook documents **how we fine-tuned `distilbert-base-uncased` on the 50 k IMDb movie-review dataset for binary sentiment** and exported the resulting weights.\n",
    "\n",
    "**Pipeline overview**\n",
    "\n",
    "1. Load and stratify-split the IMDb dataset (90 % train / 10 % validation)  \n",
    "2. Tokenise reviews with the DistilBERT tokenizer (`max_length = 256`)  \n",
    "3. Train for three epochs with the Hugging Face *Trainer* API  \n",
    "4. Evaluate (val accuracy ≈ 0.926, test accuracy ≈ 0.981)  \n",
    "5. Save the best checkpoint to `distilbert-imdb/` for deployment\n",
    "\n",
    "> *If you only need the trained model, download the release asset instead of re-running the heavy training step.*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Import the core packages for data loading, tokenisation, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, datasets, evaluate\n",
    "print(f\"torch        {torch.__version__}\")\n",
    "print(f\"transformers {transformers.__version__}\")\n",
    "print(f\"datasets     {datasets.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IMDb dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "data_path = \"imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
    "\n",
    "imdb_ds = load_dataset(\"csv\", data_files={\"train\": data_path}, split=\"train\")\n",
    "print(imdb_ds)\n",
    "print(\"\\nSample \", imdb_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split 90 % train / 10 % validation\n",
    "split_ds = imdb_ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split_ds[\"train\"]\n",
    "val_ds   = split_ds[\"test\"]\n",
    "\n",
    "print(\"Train rows:\", len(train_ds))\n",
    "print(\"Val rows  :\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    batch[\"label\"] = [label2id[s] for s in batch[\"sentiment\"]]\n",
    "    enc = tokenizer(batch[\"review\"], padding=\"max_length\",\n",
    "                    truncation=True, max_length=256)\n",
    "    batch.update(enc); return batch\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_batch, batched=True,\n",
    "                               remove_columns=[\"review\", \"sentiment\"])\n",
    "tokenized_val   =  val_ds.map(preprocess_batch, batched=True,\n",
    "                               remove_columns=[\"review\", \"sentiment\"])\n",
    "tokenized_train.set_format(\"torch\",\n",
    "                           columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_val.set_format(\"torch\",\n",
    "                         columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "#load pretrained model (num_labels=2 -> binary sentiment)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "#training hyper-parameters\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    seed=42,\n",
    "    report_to=[\"none\"]\n",
    ")\n",
    "\n",
    "#Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#start fine-tuning (on Kaggle P100)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on the validation split\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Validation metrics \", eval_metrics)\n",
    "\n",
    "#Confirm which checkpoint was judged 'best'\n",
    "print(\"Best checkpoint path \", trainer.state.best_model_checkpoint)\n",
    "\n",
    "#Save that best model for inference\n",
    "trainer.save_model(\"distilbert-imdb\")      # writes folder in /kaggle/working\n",
    "print(\"Model saved to distilbert-imdb/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#load the same tokenizer you used for training\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "#save it next to the model files\n",
    "tok.save_pretrained(\"/kaggle/working/imdb-sentiment/distilbert-imdb\")\n",
    "\n",
    "#show the final contents\n",
    "import os, glob, textwrap\n",
    "files = glob.glob(\"/kaggle/working/imdb-sentiment/distilbert-imdb/*\")\n",
    "print(textwrap.fill('\\n'.join(os.path.basename(f) for f in files), width=80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "#load model & tokenizer\n",
    "MODEL_PATH = \"imdb-sentiment/distilbert-imdb\"   # relative to working dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.eval().to(\"cpu\")                          # GPU not needed for demo\n",
    "\n",
    "label_map = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "#FastAPI app \n",
    "app = FastAPI(title=\"IMDb Sentiment API\")\n",
    "\n",
    "class Item(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(item: Item):\n",
    "    inputs = tokenizer(\n",
    "        item.text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred   = int(torch.argmax(logits, dim=-1))\n",
    "        score  = float(torch.softmax(logits, dim=-1)[0, pred])\n",
    "    return {\"label\": label_map[pred], \"confidence\": round(score, 4)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "import importlib.util, sys, pathlib\n",
    "\n",
    "# Dynamically import the api module we just wrote\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"imdb_api\", pathlib.Path(\"/kaggle/working/imdb-sentiment/api/main.py\")\n",
    ")\n",
    "api_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"imdb_api\"] = api_module\n",
    "spec.loader.exec_module(api_module)\n",
    "\n",
    "client = TestClient(api_module.app)\n",
    "\n",
    "resp = client.post(\"/predict\", json={\"text\": \"A surprisingly fun movie!\"})\n",
    "print(\"Status code:\", resp.status_code)\n",
    "print(\"Response JSON:\", resp.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_PATH = \"imdb-sentiment/distilbert-imdb\"\n",
    "\n",
    "# 1. Load test split (25 000 reviews)\n",
    "test_ds = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "# 2. Load tokenizer & model\n",
    "tok   = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(\"cuda\").eval()\n",
    "\n",
    "# 3. Batched inference\n",
    "batch_size = 64\n",
    "correct = 0\n",
    "for i in range(0, len(test_ds), batch_size):\n",
    "    texts  = test_ds[i : i + batch_size][\"text\"]\n",
    "    labels = test_ds[i : i + batch_size][\"label\"]\n",
    "    enc    = tok(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(model(**enc).logits, dim=-1).cpu().numpy()\n",
    "    correct += int(np.sum(preds == labels))\n",
    "\n",
    "test_acc = correct / len(test_ds)\n",
    "print(f\"✅ Test accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
