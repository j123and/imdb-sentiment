{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Transformer-based models like DistilBERT have revolutionized natural language processing. In this project, we explore DistilBERT's effectiveness in sentiment analysis on IMDb movie reviews. Our aim is to train a model that accurately predicts whether a review is positive or negative.","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"try:\n    import torch, transformers, datasets\n    print(\"✅ All libraries imported!\")\n    print(\"torch\", torch.__version__)\n    print(\"transformers\", transformers.__version__)\n    print(\"datasets\", datasets.__version__)\nexcept ModuleNotFoundError as e:\n    print(\"❌\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:19:09.424291Z","iopub.execute_input":"2025-08-03T14:19:09.424541Z","iopub.status.idle":"2025-08-03T14:19:12.419963Z","shell.execute_reply.started":"2025-08-03T14:19:09.424507Z","shell.execute_reply":"2025-08-03T14:19:12.419019Z"}},"outputs":[{"name":"stdout","text":"✅ All libraries imported!\ntorch 2.1.2\ntransformers 4.39.3\ndatasets 2.18.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Point to the mounted CSV\ndata_path = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n\nimdb_ds = load_dataset(\"csv\", data_files={\"train\": data_path}, split=\"train\")\nprint(imdb_ds)\nprint(\"\\nSample ➜\", imdb_ds[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:19:12.627917Z","iopub.execute_input":"2025-08-03T14:19:12.628160Z","iopub.status.idle":"2025-08-03T14:19:15.060872Z","shell.execute_reply.started":"2025-08-03T14:19:12.628139Z","shell.execute_reply":"2025-08-03T14:19:15.059926Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7cb1d9f9d642258aee5c35393f5136"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['review', 'sentiment'],\n    num_rows: 50000\n})\n\nSample ➜ {'review': \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\", 'sentiment': 'positive'}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1. Split 90 % train / 10 % validation\nsplit_ds = imdb_ds.train_test_split(test_size=0.1, seed=42)\ntrain_ds = split_ds[\"train\"]\nval_ds   = split_ds[\"test\"]\n\nprint(\"Train rows:\", len(train_ds))\nprint(\"Val rows  :\", len(val_ds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:19:19.632782Z","iopub.execute_input":"2025-08-03T14:19:19.633474Z","iopub.status.idle":"2025-08-03T14:19:19.643405Z","shell.execute_reply.started":"2025-08-03T14:19:19.633448Z","shell.execute_reply":"2025-08-03T14:19:19.642479Z"}},"outputs":[{"name":"stdout","text":"Train rows: 45000\nVal rows  : 5000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nlabel2id = {\"negative\": 0, \"positive\": 1}\n\ndef preprocess_batch(batch):\n    batch[\"label\"] = [label2id[s] for s in batch[\"sentiment\"]]\n    enc = tokenizer(batch[\"review\"], padding=\"max_length\",\n                    truncation=True, max_length=256)\n    batch.update(enc); return batch\n\ntokenized_train = train_ds.map(preprocess_batch, batched=True,\n                               remove_columns=[\"review\", \"sentiment\"])\ntokenized_val   =  val_ds.map(preprocess_batch, batched=True,\n                               remove_columns=[\"review\", \"sentiment\"])\ntokenized_train.set_format(\"torch\",\n                           columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntokenized_val.set_format(\"torch\",\n                         columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:19:24.078649Z","iopub.execute_input":"2025-08-03T14:19:24.078984Z","iopub.status.idle":"2025-08-03T14:19:50.648766Z","shell.execute_reply.started":"2025-08-03T14:19:24.078958Z","shell.execute_reply":"2025-08-03T14:19:50.647884Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f31b45e8fa8f4adcb2b966e76d069c1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e279a51bcc644cf2af3f115dd53d4dab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecdc14e5b334d89a2466fd3c3b61ae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"769141c5e267492cbeaf7ea93ddbd479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b875a9d75fa342d4a60f378f06256a9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97fbf02bde5846e9873fa2f6116de04e"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import (\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport numpy as np\nimport evaluate\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\n\n# 1️⃣  load pretrained model (num_labels=2 -> binary sentiment)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,\n)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return accuracy_metric.compute(predictions=preds, references=labels)\n\n# 3️⃣  training hyper-parameters\nargs = TrainingArguments(\n    output_dir=\"checkpoints\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    seed=42,\n    report_to=[\"none\"]\n)\n\n# 4️⃣  Trainer object\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    compute_metrics=compute_metrics,\n)\n\n# 5️⃣  start fine-tuning (≈2 h on Kaggle P100)\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:21:01.103747Z","iopub.execute_input":"2025-08-03T14:21:01.104126Z","iopub.status.idle":"2025-08-03T14:49:44.688599Z","shell.execute_reply.started":"2025-08-03T14:21:01.104095Z","shell.execute_reply":"2025-08-03T14:49:44.687807Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3efd708f2e44978264a16994fd8df7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b37e9308d0a4e858c75efd2b39eb599"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8439' max='8439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8439/8439 28:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.220200</td>\n      <td>0.229818</td>\n      <td>0.917800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.145100</td>\n      <td>0.302815</td>\n      <td>0.912400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.094900</td>\n      <td>0.307398</td>\n      <td>0.926200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8439, training_loss=0.1759010926145049, metrics={'train_runtime': 1720.0413, 'train_samples_per_second': 78.486, 'train_steps_per_second': 4.906, 'total_flos': 8941549409280000.0, 'train_loss': 0.1759010926145049, 'epoch': 3.0})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 1️⃣  Evaluate on the validation split\neval_metrics = trainer.evaluate()\nprint(\"Validation metrics ➜\", eval_metrics)\n\n# 2️⃣  Confirm which checkpoint was judged 'best'\nprint(\"Best checkpoint path ➜\", trainer.state.best_model_checkpoint)\n\n# 3️⃣  Save that best model for inference\ntrainer.save_model(\"distilbert-imdb\")      # writes folder in /kaggle/working\nprint(\"✅ Model saved to distilbert-imdb/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:52:32.498989Z","iopub.execute_input":"2025-08-03T14:52:32.499352Z","iopub.status.idle":"2025-08-03T14:52:52.264102Z","shell.execute_reply.started":"2025-08-03T14:52:32.499328Z","shell.execute_reply":"2025-08-03T14:52:52.263142Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation metrics ➜ {'eval_loss': 0.3073979616165161, 'eval_accuracy': 0.9262, 'eval_runtime': 19.2223, 'eval_samples_per_second': 260.115, 'eval_steps_per_second': 16.283, 'epoch': 3.0}\nBest checkpoint path ➜ checkpoints/checkpoint-8439\n✅ Model saved to distilbert-imdb/\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!zip -r distilbert-imdb.zip distilbert-imdb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:53:14.058288Z","iopub.execute_input":"2025-08-03T14:53:14.058716Z","iopub.status.idle":"2025-08-03T14:53:28.613456Z","shell.execute_reply.started":"2025-08-03T14:53:14.058689Z","shell.execute_reply":"2025-08-03T14:53:28.612532Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: distilbert-imdb/ (stored 0%)\n  adding: distilbert-imdb/training_args.bin (deflated 51%)\n  adding: distilbert-imdb/model.safetensors (deflated 8%)\n  adding: distilbert-imdb/config.json (deflated 46%)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!ls -R /kaggle/working/distilbert-imdb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:55:58.335307Z","iopub.execute_input":"2025-08-03T14:55:58.335801Z","iopub.status.idle":"2025-08-03T14:55:59.385801Z","shell.execute_reply.started":"2025-08-03T14:55:58.335767Z","shell.execute_reply":"2025-08-03T14:55:59.384701Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working/distilbert-imdb:\nconfig.json  model.safetensors\ttraining_args.bin\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# 1️⃣  create the project directory\n!mkdir -p /kaggle/working/imdb-sentiment\n\n# 2️⃣  copy the model folder into that project\n!cp -r /kaggle/working/distilbert-imdb /kaggle/working/imdb-sentiment/\n\n# 3️⃣  show the new layout\n!ls -R /kaggle/working/imdb-sentiment\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:56:52.525934Z","iopub.execute_input":"2025-08-03T14:56:52.526313Z","iopub.status.idle":"2025-08-03T14:56:55.884328Z","shell.execute_reply.started":"2025-08-03T14:56:52.526282Z","shell.execute_reply":"2025-08-03T14:56:55.883216Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working/imdb-sentiment:\ndistilbert-imdb\n\n/kaggle/working/imdb-sentiment/distilbert-imdb:\nconfig.json  model.safetensors\ttraining_args.bin\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# 1️⃣  load the same tokenizer you used for training\ntok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# 2️⃣  save it next to the model files\ntok.save_pretrained(\"/kaggle/working/imdb-sentiment/distilbert-imdb\")\n\n# 3️⃣  show the final contents\nimport os, glob, textwrap\nfiles = glob.glob(\"/kaggle/working/imdb-sentiment/distilbert-imdb/*\")\nprint(textwrap.fill('\\n'.join(os.path.basename(f) for f in files), width=80))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:58:04.563499Z","iopub.execute_input":"2025-08-03T14:58:04.563868Z","iopub.status.idle":"2025-08-03T14:58:04.860438Z","shell.execute_reply.started":"2025-08-03T14:58:04.563839Z","shell.execute_reply":"2025-08-03T14:58:04.859487Z"}},"outputs":[{"name":"stdout","text":"training_args.bin model.safetensors tokenizer_config.json vocab.txt\nspecial_tokens_map.json config.json tokenizer.json\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%bash\nmkdir -p /kaggle/working/imdb-sentiment/api\n\ncat > /kaggle/working/imdb-sentiment/api/main.py <<'PY'\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# --- load model & tokenizer -----------------------------------------------\nMODEL_PATH = \"imdb-sentiment/distilbert-imdb\"   # relative to working dir\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel     = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\nmodel.eval().to(\"cpu\")                          # GPU not needed for demo\n\nlabel_map = {0: \"negative\", 1: \"positive\"}\n\n# --- FastAPI app ----------------------------------------------------------\napp = FastAPI(title=\"IMDb Sentiment API\")\n\nclass Item(BaseModel):\n    text: str\n\n@app.post(\"/predict\")\ndef predict(item: Item):\n    inputs = tokenizer(\n        item.text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256,\n        return_tensors=\"pt\",\n    )\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        pred   = int(torch.argmax(logits, dim=-1))\n        score  = float(torch.softmax(logits, dim=-1)[0, pred])\n    return {\"label\": label_map[pred], \"confidence\": round(score, 4)}\nPY\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:59:09.263414Z","iopub.execute_input":"2025-08-03T14:59:09.264040Z","iopub.status.idle":"2025-08-03T14:59:09.282638Z","shell.execute_reply.started":"2025-08-03T14:59:09.264012Z","shell.execute_reply":"2025-08-03T14:59:09.281404Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"ls /kaggle/working/imdb-sentiment/api/main.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:59:19.476233Z","iopub.execute_input":"2025-08-03T14:59:19.476913Z","iopub.status.idle":"2025-08-03T14:59:20.507186Z","shell.execute_reply.started":"2025-08-03T14:59:19.476882Z","shell.execute_reply":"2025-08-03T14:59:20.506292Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working/imdb-sentiment/api/main.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install -q fastapi uvicorn[standard] \"requests>=2.31\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:59:54.674566Z","iopub.execute_input":"2025-08-03T14:59:54.674916Z","iopub.status.idle":"2025-08-03T15:00:03.252567Z","shell.execute_reply.started":"2025-08-03T14:59:54.674889Z","shell.execute_reply":"2025-08-03T15:00:03.251381Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from fastapi.testclient import TestClient\nimport importlib.util, sys, pathlib\n\n# Dynamically import the api module we just wrote\nspec = importlib.util.spec_from_file_location(\n    \"imdb_api\", pathlib.Path(\"/kaggle/working/imdb-sentiment/api/main.py\")\n)\napi_module = importlib.util.module_from_spec(spec)\nsys.modules[\"imdb_api\"] = api_module\nspec.loader.exec_module(api_module)\n\nclient = TestClient(api_module.app)\n\nresp = client.post(\"/predict\", json={\"text\": \"A surprisingly fun movie!\"})\nprint(\"Status code:\", resp.status_code)\nprint(\"Response JSON:\", resp.json())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T15:00:13.676478Z","iopub.execute_input":"2025-08-03T15:00:13.677134Z","iopub.status.idle":"2025-08-03T15:00:14.874163Z","shell.execute_reply.started":"2025-08-03T15:00:13.677099Z","shell.execute_reply":"2025-08-03T15:00:14.873268Z"}},"outputs":[{"name":"stdout","text":"Status code: 200\nResponse JSON: {'label': 'positive', 'confidence': 0.9908}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/imdb-sentiment\n\n# ── requirements.txt ──\ncat > requirements.txt <<'REQ'\nfastapi\nuvicorn[standard]\ntorch>=2.2\ntransformers>=4.41\nREQ\n\n# ── Dockerfile ──\ncat > Dockerfile <<'DOCK'\n# ---- base image ----\nFROM python:3.11-slim\n\n# ---- install deps ----\nCOPY requirements.txt /tmp/\nRUN pip install --no-cache-dir -r /tmp/requirements.txt\n\n# ---- copy app & model ----\nCOPY api/ /app/api/\nCOPY distilbert-imdb/ /app/distilbert-imdb/\n\n# ---- expose + run ----\nWORKDIR /app\nENV PYTHONUNBUFFERED=1\nCMD [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\nDOCK\n\nls -1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T15:00:55.045834Z","iopub.execute_input":"2025-08-03T15:00:55.046214Z","iopub.status.idle":"2025-08-03T15:00:55.066554Z","shell.execute_reply.started":"2025-08-03T15:00:55.046185Z","shell.execute_reply":"2025-08-03T15:00:55.065768Z"}},"outputs":[{"name":"stdout","text":"Dockerfile\napi\ndistilbert-imdb\nrequirements.txt\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!zip -r imdb-sentiment.zip imdb-sentiment\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T15:04:08.448421Z","iopub.execute_input":"2025-08-03T15:04:08.449399Z","iopub.status.idle":"2025-08-03T15:04:23.213879Z","shell.execute_reply.started":"2025-08-03T15:04:08.449369Z","shell.execute_reply":"2025-08-03T15:04:23.212989Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: imdb-sentiment/ (stored 0%)\n  adding: imdb-sentiment/requirements.txt (stored 0%)\n  adding: imdb-sentiment/Dockerfile (deflated 36%)\n  adding: imdb-sentiment/distilbert-imdb/ (stored 0%)\n  adding: imdb-sentiment/distilbert-imdb/training_args.bin (deflated 51%)\n  adding: imdb-sentiment/distilbert-imdb/model.safetensors (deflated 8%)\n  adding: imdb-sentiment/distilbert-imdb/tokenizer_config.json (deflated 76%)\n  adding: imdb-sentiment/distilbert-imdb/vocab.txt (deflated 53%)\n  adding: imdb-sentiment/distilbert-imdb/special_tokens_map.json (deflated 42%)\n  adding: imdb-sentiment/distilbert-imdb/config.json (deflated 46%)\n  adding: imdb-sentiment/distilbert-imdb/tokenizer.json (deflated 71%)\n  adding: imdb-sentiment/api/ (stored 0%)\n  adding: imdb-sentiment/api/main.py (deflated 52%)\n  adding: imdb-sentiment/api/__pycache__/ (stored 0%)\n  adding: imdb-sentiment/api/__pycache__/main.cpython-310.pyc (deflated 31%)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from datasets import load_dataset\nimport numpy as np, torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nMODEL_PATH = \"/kaggle/working/imdb-sentiment/distilbert-imdb\"\n\n# 1. Load test split (25 000 reviews)\ntest_ds = load_dataset(\"imdb\", split=\"test\")\n\n# 2. Load tokenizer & model\ntok   = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(\"cuda\").eval()\n\n# 3. Batched inference\nbatch_size = 64\ncorrect = 0\nfor i in range(0, len(test_ds), batch_size):\n    texts  = test_ds[i : i + batch_size][\"text\"]\n    labels = test_ds[i : i + batch_size][\"label\"]\n    enc    = tok(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        preds = torch.argmax(model(**enc).logits, dim=-1).cpu().numpy()\n    correct += int(np.sum(preds == labels))\n\ntest_acc = correct / len(test_ds)\nprint(f\"✅ Test accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T15:08:24.264566Z","iopub.execute_input":"2025-08-03T15:08:24.265436Z","iopub.status.idle":"2025-08-03T15:10:35.617459Z","shell.execute_reply.started":"2025-08-03T15:08:24.265402Z","shell.execute_reply":"2025-08-03T15:10:35.616567Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773bb6f637224dcf958ddf632802bc92"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 39.6MB/s]\nDownloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 42.3MB/s]\nDownloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 78.5MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891d848663c1497ea7f2ac8baf7651d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9829ed1d2d41998a30c0c4f741543e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79de21248884c0d91f960ef5e229b27"}},"metadata":{}},{"name":"stdout","text":"✅ Test accuracy: 0.9806\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import sys, torch, transformers, datasets\nprint(\"python:\", sys.version)\nprint(\"torch:\", torch.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"datasets:\", datasets.__version__)\nprint(\"transformers location:\", transformers.__file__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U evaluate --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:20:44.410472Z","iopub.execute_input":"2025-08-03T14:20:44.411346Z","iopub.status.idle":"2025-08-03T14:20:54.304160Z","shell.execute_reply.started":"2025-08-03T14:20:44.411315Z","shell.execute_reply":"2025-08-03T14:20:54.303024Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, DatasetDict, load_metric\nfrom transformers import pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Dataset & EDA","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.sample(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualising Target Values","metadata":{}},{"cell_type":"code","source":"df['sentiment'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\n\nplt.figure(figsize=(3, 6))\nsns.countplot(x='sentiment', data=df, palette='Set2')\nplt.title('Distribution of Sentiment Labels', fontsize=16)\nplt.xlabel('Sentiment', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_counts = df['sentiment'].value_counts()\ncolors = ['#66c2a5', '#fc8d62']\n\nplt.figure(figsize=(8, 6))\nplt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)\nplt.title('Distribution of Sentiment Labels')\nplt.axis('equal')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model (DistilBERT)","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"reviews = df['review'].tolist()\nlabels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting the Dataset","metadata":{}},{"cell_type":"code","source":"train_reviews, val_reviews, train_labels, val_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for tokenizing the reviews\n\ndef tokenize_function(texts):\n    return tokenizer(texts, padding=\"max_length\", truncation=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_encodings = tokenize_function(train_reviews)\nval_encodings = tokenize_function(val_reviews)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to Hugging Face Dataset format\n\ntrain_dataset = Dataset.from_dict({\n                                    'input_ids': train_encodings['input_ids'],\n                                    'attention_mask': train_encodings['attention_mask'],\n                                    'labels': train_labels\n                                    })\n\nval_dataset = Dataset.from_dict({\n                                    'input_ids': val_encodings['input_ids'],\n                                    'attention_mask': val_encodings['attention_mask'],\n                                    'labels': val_labels\n                                    })\n\ndataset = DatasetDict({\n                        'train': train_dataset,\n                        'validation': val_dataset\n                        })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english', num_labels=2).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load F1 metric\nf1_metric = load_metric(\"f1\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the evaluation metric\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Calculate accuracy\n    accuracy = (preds == labels).mean()\n    \n    # Calculate F1-score (weighted)\n    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n    \n    return {\"accuracy\": accuracy, \"f1\": f1[\"f1\"]}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameters Settings","metadata":{}},{"cell_type":"code","source":"# Define training arguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                     # Output directory for model checkpoints\n    num_train_epochs=5,                         # Number of training epochs\n    per_device_train_batch_size=16,             # Batch size per device (GPU/CPU)\n    per_device_eval_batch_size=16,              # Evaluation batch size\n    warmup_steps=100,                           # Number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                          # Weight decay for regularization\n    logging_dir='./logs',                       # Directory for logging\n    logging_steps=10,                           # Interval for logging updates\n    evaluation_strategy='epoch',                # Evaluate at each epoch\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,                # Load the best model based on eval_loss\n    metric_for_best_model=\"eval_loss\",          # Use validation loss to determine the best model\n    greater_is_better=False,                    # Lower validation loss is better\n    report_to=\"none\",                           # Disable reporting to Hugging Face Hub\n    push_to_hub=False,                          # Do not push to Hugging Face Hub\n    fp16=True,                                  # Enable mixed precision\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tuning the Model","metadata":{}},{"cell_type":"code","source":"# Initialize Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\neval_results = trainer.evaluate()\nprint(eval_results)\nprint()\nprint(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\nprint(f\"F1-Score: {eval_results['eval_f1']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"In our experiment with DistilBERT on IMDb reviews, we achieved a peak accuracy score of 0.9371 and an F1-score of 0.9371, demonstrating the effectiveness of transformer-based models for sentiment analysis. By fine-tuning DistilBERT with optimized hyperparameters, including validation loss as the metric for model selection, we balanced performance and generalization efficiently.\n\nThe model’s consistent improvements in accuracy and F1-score, despite slight increases in validation loss, highlight the importance of monitoring multiple metrics. With its efficiency and adaptability, DistilBERT proves to be a robust solution for text classification tasks, offering a strong foundation for further exploration and refinement.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}